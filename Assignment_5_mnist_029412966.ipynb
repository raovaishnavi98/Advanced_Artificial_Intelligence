{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading mnist dataset\n",
    "from mnist.loader import MNIST\n",
    "\n",
    "mndata = MNIST('mnist_data')\n",
    "\n",
    "X_train, y_train = mndata.load_training()\n",
    "X_test, y_test = mndata.load_testing()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 784)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "id = 7\n",
    "\n",
    "image = np.array(X_train[id], dtype='float')\n",
    "pixels = image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# print(X_train[id])\n",
    "print(y_train[id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert dataset to numpy array and outputs to categorical\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to split train set and validation set 1/6 randomly for each call\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import random\n",
    "\n",
    "def train_valid_split(X_data, y_data):\n",
    "    return train_test_split(X_data, y_data, test_size=0.166, random_state=random.randint(100), shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) Optimizer evaluation:\n",
    "\n",
    "Optimal Learning Rates for all optimizers was found by using -\n",
    "lr = random.choice([1.0,0.1,0.01,0.001,0.0001]) and training the model 10 times with random sub_train and validation sets.\n",
    "After obtaining optimal learning rate, the remaining hyper-parameters were tuned by training the model 10 times with different random sub_train and validation sets. This is shown below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random values for beta_1, beta_2 and batch_size are: 0.976221150238183, 0.9989042907844174 and 40\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3038 - accuracy: 0.9298\n",
      "Random values for beta_1, beta_2 and batch_size are: 0.9331641596755733, 0.9951318251038282 and 65\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2739 - accuracy: 0.9337\n",
      "Random values for beta_1, beta_2 and batch_size are: 0.8643752225613837, 0.9971608946010938 and 49\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2852 - accuracy: 0.9252\n",
      "Random values for beta_1, beta_2 and batch_size are: 0.9679402388784674, 0.9902852370529855 and 72\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2727 - accuracy: 0.9350\n",
      "Random values for beta_1, beta_2 and batch_size are: 0.9018317861334962, 0.992954707284092 and 65\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2829 - accuracy: 0.9343\n",
      "Random values for beta_1, beta_2 and batch_size are: 0.9703710549082487, 0.9996229957918663 and 98\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3917 - accuracy: 0.8987\n",
      "Random values for beta_1, beta_2 and batch_size are: 0.9216644441041758, 0.9958818998626273 and 66\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.9289\n",
      "Random values for beta_1, beta_2 and batch_size are: 0.9616111502361786, 0.9917437596125109 and 88\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3254 - accuracy: 0.9231\n",
      "Random values for beta_1, beta_2 and batch_size are: 0.9284189625520035, 0.994260686266452 and 88\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2875 - accuracy: 0.9357\n",
      "Random values for beta_1, beta_2 and batch_size are: 0.9307379506636834, 0.9947552957192553 and 49\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.9404\n",
      "Accuracy:  92.85\n"
     ]
    }
   ],
   "source": [
    "# Using 32U single layer model for optimizers evaluation and tuning hyper-parameters to report accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "# For ADAM\n",
    "accuracy_list = []\n",
    "\n",
    "for iteration in range(10):\n",
    "    sub_train_x, sub_val_x, sub_train_y, sub_val_y = train_valid_split(X_train, y_train)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    b1=random.uniform(0.8,1)\n",
    "    b2=random.uniform(0.990,1)\n",
    "    batch_no = random.randint(35,100)\n",
    "    print(f\"Random values for beta_1, beta_2 and batch_size are: {b1}, {b2} and {batch_no}\")\n",
    "    # Learning rate for Adam has the optimal value 0.001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=b1, beta_2=b2, epsilon=1e-07)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=batch_no, verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    accuracy_list.append(results[1])\n",
    "\n",
    "# Reporting test accuracy\n",
    "import statistics\n",
    "acc_mean = statistics.mean(accuracy_list)\n",
    "acc_result = acc_mean*100\n",
    "acc_result = round(acc_result, 2)\n",
    "print(\"Accuracy: \", acc_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random values for rho and batch_size are: 0.9962380849539331 and 49\n",
      "313/313 [==============================] - 0s 944us/step - loss: 0.3772 - accuracy: 0.8974\n",
      "Random values for rho and batch_size are: 0.8516611950595442 and 41\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.7423 - accuracy: 0.9035\n",
      "Random values for rho and batch_size are: 0.8382874393539324 and 61\n",
      "313/313 [==============================] - 0s 896us/step - loss: 0.5551 - accuracy: 0.9229\n",
      "Random values for rho and batch_size are: 0.9289500832432762 and 57\n",
      "313/313 [==============================] - 0s 920us/step - loss: 0.4143 - accuracy: 0.9336\n",
      "Random values for rho and batch_size are: 0.9825210330277548 and 56\n",
      "313/313 [==============================] - 0s 968us/step - loss: 0.3167 - accuracy: 0.9345\n",
      "Random values for rho and batch_size are: 0.891681644332136 and 38\n",
      "313/313 [==============================] - 0s 936us/step - loss: 0.5847 - accuracy: 0.9107\n",
      "Random values for rho and batch_size are: 0.8155171109511278 and 85\n",
      "313/313 [==============================] - 0s 949us/step - loss: 0.5800 - accuracy: 0.9258\n",
      "Random values for rho and batch_size are: 0.9964088143312196 and 88\n",
      "313/313 [==============================] - 0s 998us/step - loss: 0.4406 - accuracy: 0.8960\n",
      "Random values for rho and batch_size are: 0.9128568105801886 and 56\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.5191 - accuracy: 0.9255\n",
      "Random values for rho and batch_size are: 0.9573527966211481 and 67\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.9322\n",
      "Accuracy:  91.82\n"
     ]
    }
   ],
   "source": [
    "# For RMSProp\n",
    "accuracy_list = []\n",
    "\n",
    "for iteration in range(10):\n",
    "    sub_train_x, sub_val_x, sub_train_y, sub_val_y = train_valid_split(X_train, y_train)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    r=random.uniform(0.8,1)\n",
    "    batch_no = random.randint(35,100)\n",
    "    print(f\"Random values for rho and batch_size are: {r} and {batch_no}\")\n",
    "    # Learning rate for RMSProp has the optimal value 0.001\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=r, momentum=0.0, epsilon=1e-07)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=batch_no, verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    accuracy_list.append(results[1])\n",
    "\n",
    "# Reporting test accuracy\n",
    "import statistics\n",
    "acc_mean = statistics.mean(accuracy_list)\n",
    "acc_result = acc_mean*100\n",
    "acc_result = round(acc_result, 2)\n",
    "print(\"Accuracy: \", acc_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random value for batch_size is: 71\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.7602 - accuracy: 0.8107\n",
      "Random value for batch_size is: 83\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.9160 - accuracy: 0.7235\n",
      "Random value for batch_size is: 46\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.8190 - accuracy: 0.7609\n",
      "Random value for batch_size is: 58\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6070 - accuracy: 0.8525\n",
      "Random value for batch_size is: 69\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6201 - accuracy: 0.8078\n",
      "Random value for batch_size is: 79\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.8645\n",
      "Random value for batch_size is: 89\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5643 - accuracy: 0.8634\n",
      "Random value for batch_size is: 69\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.8031\n",
      "Random value for batch_size is: 87\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.7239 - accuracy: 0.8130\n",
      "Random value for batch_size is: 65\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6411 - accuracy: 0.8461\n",
      "Accuracy:  81.46\n"
     ]
    }
   ],
   "source": [
    "# For SGD\n",
    "accuracy_list = []\n",
    "\n",
    "for iteration in range(10):\n",
    "    sub_train_x, sub_val_x, sub_train_y, sub_val_y = train_valid_split(X_train, y_train)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    batch_no = random.randint(35,100)\n",
    "    print(f\"Random value for batch_size is: {batch_no}\")\n",
    "    # Learning rate for SGD has the optimal value 0.001\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.0)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=batch_no, verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    accuracy_list.append(results[1])\n",
    "\n",
    "# Reporting test accuracy\n",
    "import statistics\n",
    "acc_mean = statistics.mean(accuracy_list)\n",
    "acc_result = acc_mean*100\n",
    "acc_result = round(acc_result, 2)\n",
    "print(\"Accuracy: \", acc_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random value for batch_size is: 77\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5574 - accuracy: 0.8745\n",
      "Random value for batch_size is: 48\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.8485\n",
      "Random value for batch_size is: 79\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.7134 - accuracy: 0.8174\n",
      "Random value for batch_size is: 73\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4953 - accuracy: 0.8851\n",
      "Random value for batch_size is: 43\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.7026 - accuracy: 0.7845\n",
      "Random value for batch_size is: 35\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5825 - accuracy: 0.8635\n",
      "Random value for batch_size is: 58\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6095 - accuracy: 0.8746\n",
      "Random value for batch_size is: 68\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.7342 - accuracy: 0.8169\n",
      "Random value for batch_size is: 86\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5335 - accuracy: 0.8517\n",
      "Random value for batch_size is: 61\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.8522\n",
      "Accuracy:  84.69\n"
     ]
    }
   ],
   "source": [
    "# For Adagrad\n",
    "accuracy_list = []\n",
    "\n",
    "for iteration in range(10):\n",
    "    sub_train_x, sub_val_x, sub_train_y, sub_val_y = train_valid_split(X_train, y_train)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    batch_no = random.randint(35,100)\n",
    "    print(f\"Random value for batch_size is: {batch_no}\")\n",
    "    # Learning rate for Adagrad has the optimal value 0.01\n",
    "    optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.01, initial_accumulator_value=0.1, epsilon=1e-07)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=batch_no, verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    accuracy_list.append(results[1])\n",
    "\n",
    "# Reporting test accuracy\n",
    "import statistics\n",
    "acc_mean = statistics.mean(accuracy_list)\n",
    "acc_result = acc_mean*100\n",
    "acc_result = round(acc_result, 2)\n",
    "print(\"Accuracy: \", acc_result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accuracies of optimizers after tuning hyper-parameters are:\n",
    "ADAM - 91.92\n",
    "RMSProp - 91.82\n",
    "SGD - 81.46\n",
    "Adagrad - 84.69\n",
    "\n",
    "The best optimizer of choice is ADAM with fine tuned parameters - learning_rate = 0.0001, beta_1 = 0.93, beta_2 = 0.995, batch_size = 49.\n",
    "\n",
    "d) Finding 3 best architectures:\n",
    "Here, size of the layer and activation functions are chosen at random for 3 architectures having 4 layers, 6 layers and 10 layers respectively."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size = 64, activation = tanh\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2895 - accuracy: 0.9107\n",
      "layer size = 128, activation = sigmoid\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3107 - accuracy: 0.9041\n",
      "layer size = 128, activation = sigmoid\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3034 - accuracy: 0.9102\n",
      "layer size = 16, activation = relu\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3363 - accuracy: 0.8970\n",
      "layer size = 64, activation = relu\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3014 - accuracy: 0.9060\n",
      "layer size = 64, activation = relu\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2947 - accuracy: 0.9104\n",
      "layer size = 128, activation = relu\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2886 - accuracy: 0.9072\n",
      "layer size = 32, activation = tanh\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3269 - accuracy: 0.8971\n",
      "layer size = 32, activation = relu\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3059 - accuracy: 0.9062\n",
      "layer size = 32, activation = tanh\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3089 - accuracy: 0.9067\n",
      "Accuracy:  90.56\n"
     ]
    }
   ],
   "source": [
    "# Find 3 best architectures in terms of test accuracy\n",
    "\n",
    "# Architecture 1 - 4 layers\n",
    "accuracy_list = []\n",
    "\n",
    "layer_size = [128,64,32,16]\n",
    "activation_fn = ['relu', 'tanh', 'sigmoid']\n",
    "for iteration in range(10):\n",
    "    sub_train_x, sub_val_x, sub_train_y, sub_val_y = train_valid_split(X_train, y_train)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    l2_size = random.choice(layer_size)\n",
    "    l2_activation = random.choice(activation_fn)\n",
    "    print(f\"layer size = {l2_size}, activation = {l2_activation}\")\n",
    "    model.add(Dense(l2_size, activation=l2_activation))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.93, beta_2=0.995, epsilon=1e-07)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=49, verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    accuracy_list.append(results[1])\n",
    "\n",
    "# Reporting test accuracy\n",
    "import statistics\n",
    "acc_mean = statistics.mean(accuracy_list)\n",
    "acc_result = acc_mean*100\n",
    "acc_result = round(acc_result, 2)\n",
    "print(\"Accuracy: \", acc_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above run shows the best architecture for 4 layers -\n",
    "x -> relu(32) -> tanh(64)-> relu(16) -> softmax(10) -> y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size = 16, activation = relu\n",
      "layer size = 64, activation = relu\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.9266\n",
      "layer size = 32, activation = softmax\n",
      "layer size = 64, activation = relu\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.9083\n",
      "layer size = 128, activation = softmax\n",
      "layer size = 128, activation = relu\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2700 - accuracy: 0.9192\n",
      "layer size = 16, activation = relu\n",
      "layer size = 64, activation = relu\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2627 - accuracy: 0.9223\n",
      "layer size = 64, activation = softmax\n",
      "layer size = 16, activation = softmax\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3087 - accuracy: 0.9121\n",
      "layer size = 32, activation = softmax\n",
      "layer size = 32, activation = softmax\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3301 - accuracy: 0.9049\n",
      "layer size = 64, activation = softmax\n",
      "layer size = 64, activation = softmax\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.9000\n",
      "layer size = 16, activation = tanh\n",
      "layer size = 128, activation = relu\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2678 - accuracy: 0.9204\n",
      "layer size = 128, activation = tanh\n",
      "layer size = 128, activation = softmax\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.9213\n",
      "layer size = 16, activation = relu\n",
      "layer size = 128, activation = tanh\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.9251\n",
      "Accuracy:  91.6\n"
     ]
    }
   ],
   "source": [
    "# Architecture 2 - 6 layers\n",
    "accuracy_list = []\n",
    "\n",
    "layer_size = [128,64,32,16]\n",
    "activation_fn = ['relu', 'tanh', 'softmax']\n",
    "for iteration in range(10):\n",
    "    sub_train_x, sub_val_x, sub_train_y, sub_val_y = train_valid_split(X_train, y_train)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(128, activation='tanh'))\n",
    "    l3_size = random.choice(layer_size)\n",
    "    l3_activation = random.choice(activation_fn)\n",
    "    print(f\"layer size = {l3_size}, activation = {l3_activation}\")\n",
    "    model.add(Dense(l3_size, activation=l3_activation))\n",
    "    l4_size = random.choice(layer_size)\n",
    "    l4_activation = random.choice(activation_fn)\n",
    "    print(f\"layer size = {l4_size}, activation = {l4_activation}\")\n",
    "    model.add(Dense(l4_size, activation=l4_activation))\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.93, beta_2=0.995, epsilon=1e-07)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=49, verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    accuracy_list.append(results[1])\n",
    "\n",
    "# Reporting test accuracy\n",
    "import statistics\n",
    "acc_mean = statistics.mean(accuracy_list)\n",
    "acc_result = acc_mean*100\n",
    "acc_result = round(acc_result, 2)\n",
    "print(\"Accuracy: \", acc_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above run shows the best architecture for 7 layers -\n",
    "x -> relu(32) -> tanh(128) -> relu(16) -> relu(64) -> tanh(16) -> sigmoid(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer size = 16, activation = tanh\n",
      "layer size = 128, activation = tanh\n",
      "layer size = 32, activation = tanh\n",
      "layer size = 64, activation = relu\n",
      "layer size = 128, activation = relu\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4604 - accuracy: 0.8596\n",
      "layer size = 64, activation = sigmoid\n",
      "layer size = 32, activation = tanh\n",
      "layer size = 128, activation = relu\n",
      "layer size = 128, activation = tanh\n",
      "layer size = 64, activation = sigmoid\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2524 - accuracy: 0.9255\n",
      "layer size = 64, activation = relu\n",
      "layer size = 128, activation = relu\n",
      "layer size = 32, activation = sigmoid\n",
      "layer size = 32, activation = tanh\n",
      "layer size = 128, activation = relu\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.6587 - accuracy: 0.7573\n",
      "layer size = 32, activation = tanh\n",
      "layer size = 128, activation = sigmoid\n",
      "layer size = 16, activation = relu\n",
      "layer size = 64, activation = relu\n",
      "layer size = 16, activation = tanh\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3884 - accuracy: 0.8844\n",
      "layer size = 32, activation = sigmoid\n",
      "layer size = 16, activation = sigmoid\n",
      "layer size = 16, activation = sigmoid\n",
      "layer size = 128, activation = relu\n",
      "layer size = 128, activation = tanh\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4614 - accuracy: 0.8605\n",
      "layer size = 128, activation = relu\n",
      "layer size = 16, activation = relu\n",
      "layer size = 32, activation = tanh\n",
      "layer size = 128, activation = relu\n",
      "layer size = 16, activation = sigmoid\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.4970 - accuracy: 0.8503\n",
      "layer size = 64, activation = tanh\n",
      "layer size = 32, activation = sigmoid\n",
      "layer size = 128, activation = tanh\n",
      "layer size = 32, activation = tanh\n",
      "layer size = 32, activation = sigmoid\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3267 - accuracy: 0.8983\n",
      "layer size = 16, activation = sigmoid\n",
      "layer size = 64, activation = relu\n",
      "layer size = 32, activation = sigmoid\n",
      "layer size = 64, activation = tanh\n",
      "layer size = 32, activation = tanh\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.4227 - accuracy: 0.8774\n",
      "layer size = 128, activation = relu\n",
      "layer size = 32, activation = tanh\n",
      "layer size = 16, activation = sigmoid\n",
      "layer size = 64, activation = tanh\n",
      "layer size = 32, activation = sigmoid\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.7655 - accuracy: 0.7270\n",
      "layer size = 64, activation = relu\n",
      "layer size = 64, activation = tanh\n",
      "layer size = 32, activation = sigmoid\n",
      "layer size = 128, activation = sigmoid\n",
      "layer size = 128, activation = tanh\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2877 - accuracy: 0.9202\n",
      "Accuracy:  85.6\n"
     ]
    }
   ],
   "source": [
    "# Architecture 3 - 10 layers\n",
    "accuracy_list = []\n",
    "\n",
    "layer_size = [128,64,32,16]\n",
    "activation_fn = ['relu', 'tanh', 'sigmoid']\n",
    "for iteration in range(10):\n",
    "    sub_train_x, sub_val_x, sub_train_y, sub_val_y = train_valid_split(X_train, y_train)\n",
    "    model = Sequential()\n",
    "    l1_size = random.choice(layer_size)\n",
    "    l1_activation = random.choice(activation_fn)\n",
    "    print(f\"layer size = {l1_size}, activation = {l1_activation}\")\n",
    "    model.add(Dense(l1_size, activation=l1_activation))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    l3_size = random.choice(layer_size)\n",
    "    l3_activation = random.choice(activation_fn)\n",
    "    print(f\"layer size = {l3_size}, activation = {l3_activation}\")\n",
    "    model.add(Dense(l3_size, activation=l3_activation))\n",
    "    model.add(Dense(32, activation='tanh'))\n",
    "    l5_size = random.choice(layer_size)\n",
    "    l5_activation = random.choice(activation_fn)\n",
    "    print(f\"layer size = {l5_size}, activation = {l5_activation}\")\n",
    "    model.add(Dense(l5_size, activation=l5_activation))\n",
    "    l6_size = random.choice(layer_size)\n",
    "    l6_activation = random.choice(activation_fn)\n",
    "    print(f\"layer size = {l6_size}, activation = {l6_activation}\")\n",
    "    model.add(Dense(l6_size, activation=l6_activation))\n",
    "    model.add(Dense(16, activation='tanh'))\n",
    "    l8_size = random.choice(layer_size)\n",
    "    l8_activation = random.choice(activation_fn)\n",
    "    print(f\"layer size = {l8_size}, activation = {l8_activation}\")\n",
    "    model.add(Dense(l8_size, activation=l8_activation))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.93, beta_2=0.995, epsilon=1e-07)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=10, batch_size=49, verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    accuracy_list.append(results[1])\n",
    "\n",
    "# Reporting test accuracy\n",
    "import statistics\n",
    "acc_mean = statistics.mean(accuracy_list)\n",
    "acc_result = acc_mean*100\n",
    "acc_result = round(acc_result, 2)\n",
    "print(\"Accuracy: \", acc_result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above run shows best architecture for 10 layers -\n",
    "x -> sigmoid(64) -> relu(32) -> tanh(32) -> tanh(32) -> relu(128) -> tanh(128) -> tanh(16) -> sigmoid(64) -> relu(32) -> sigmoid(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}