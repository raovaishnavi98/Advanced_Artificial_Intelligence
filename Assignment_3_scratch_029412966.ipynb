{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.b)\n",
    "(i) Deriving $\\frac{\\partial L}{\\partial \\vec W^1} :$\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial \\vec W^1} = \\begin{bmatrix} \\frac{\\partial L}{\\partial W^1_1_1} & \\frac{\\partial L}{\\partial W^1_1_2} \\\\ \\\\ \\frac{\\partial L}{\\partial W^1_2_1} & \\frac{\\partial L}{\\partial W^1_2_2} \\end{bmatrix}$\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial W^1_1_1} = \\frac{\\partial L}{\\partial h^1_1} . \\frac{\\partial h^1_1}{\\partial a^1_1} . \\frac{\\partial a^1_1}{\\partial W^1_1_1}$\n",
    "\\\n",
    "$ = \\frac{\\partial L}{\\partial h^2_1}.\\frac{\\partial h^2_1}{\\partial a^2_1}.\\frac{\\partial a^2_1}{\\partial h^1_1} . \\frac{\\partial h^1_1}{\\partial a^1_1} . \\frac{\\partial a^1_1}{\\partial W^1_1_1}$\n",
    "\\\n",
    "$ = - \\frac{y_1}{\\hat y_1} + \\frac{1 - y_1}{1 - \\hat y_1} . \\hat y_1 (1 - \\hat y_1) w^2_1_1 (1 - (h^1_1)^2 x_1$\n",
    "\\\n",
    "$ = (\\hat y_1 - y_1) w^2_1_1 (1 - (h^1_1)^2 x_1$\n",
    "\\\n",
    "\\\n",
    "Similarly,\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial W^1_2_1} = (\\hat y_1 - y_1) w^2_1_1 (1 - (h^1_1)^2 x_2$\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial W^1_1_2} = (\\hat y_1 - y_1) w^2_2_1 (1 - (h^1_2)^2 x_1$\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial W^1_2_2} = (\\hat y_1 - y_1) w^2_2_1 (1 - (h^1_2)^2 x_2$\n",
    "\\\n",
    "The matrix becomes:\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial \\vec W^1} = \\begin{bmatrix} (\\hat y_1 - y_1) w^2_1_1 (1 - (h^1_1)^2 x_1 & (\\hat y_1 - y_1) w^2_2_1 (1 - (h^1_2)^2 x_1 \\\\ \\\\ (\\hat y_1 - y_1) w^2_1_1 (1 - (h^1_1)^2 x_2 & (\\hat y_1 - y_1) w^2_2_1 (1 - (h^1_2)^2 x_2 \\end{bmatrix}$\n",
    "\\\n",
    "\\\n",
    "(ii) Deriving $\\frac{\\partial L}{\\partial \\vec w^2} :$\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial \\vec w^2} = \\begin{bmatrix} \\frac{\\partial L}{\\partial W^2_1_1} \\\\ \\\\ \\frac{\\partial L}{\\partial W^2_2_1} \\end{bmatrix}$\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial W^2_1_1} = \\frac{\\partial L}{\\partial h^2_1} . \\frac{\\partial h^2_1}{\\partial a^2_1} . \\frac{\\partial a^2_1}{\\partial W^2_1_1}$\n",
    "\\\n",
    "$ = - \\frac{y_1}{\\hat y_1} + \\frac{1 - y_1}{1 - \\hat y_1} . \\hat y_1 (1 - \\hat y_1) h^1_1$\n",
    "\\\n",
    "$ = (\\hat y_1 - y_1) h^1_1$\n",
    "\\\n",
    "\\\n",
    "Similarly,\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial W^2_2_1} = (\\hat y_1 - y_1) h^1_2$\n",
    "\\\n",
    "The matrix becomes:\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial \\vec w^2} = \\begin{bmatrix} (\\hat y_1 - y_1) h^1_1 \\\\ \\\\ (\\hat y_1 - y_1) h^1_2 \\end{bmatrix}$\n",
    "\\\n",
    "\\\n",
    "(iii) Deriving $\\frac{\\partial L}{\\partial \\vec b^1} :$\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial \\vec b^1} = \\frac{\\partial L}{\\partial h^2_1} . \\frac{\\partial h^2_1}{\\partial a^2_1} . \\frac{\\partial a^2_1}{\\partial h^1_1} . \\frac{\\partial h^1_1}{\\partial a^1_1} . \\frac{\\partial a^1_1}{\\partial b^1_1}$\n",
    "\\\n",
    "$ = - \\frac{y_1}{\\hat y_1} + \\frac{1 - y_1}{1 - \\hat y_1} . \\hat y_1 (1 - \\hat y_1) w^2_1_1 (1 - (h^1_1)^2 . 1$\n",
    "\\\n",
    "Thus,\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial \\vec b^1} = (\\hat y_1 - y_1) w^2_1_1 (1 - (h^1_1)^2$\n",
    "\\\n",
    "\\\n",
    "(iv) Deriving $\\frac{\\partial L}{\\partial b^2} :$\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial \\vec b^1} = \\frac{\\partial L}{\\partial h^2_1} . \\frac{\\partial h^2_1}{\\partial a^2_1} . \\frac{\\partial a^2_1}{\\partial b^2}$\n",
    "\\\n",
    "$ = - \\frac{y_1}{\\hat y_1} + \\frac{1 - y_1}{1 - \\hat y_1} . \\hat y_1 (1 - \\hat y_1)$\n",
    "\\\n",
    "Thus,\n",
    "\\\n",
    "$\\frac{\\partial L}{\\partial b^2} = (\\hat y_1 - y_1) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output:\n",
      " [array([[0.01504229]]), array([[0.99144936]]), array([[0.99142579]]), array([[0.01634774]])] \n",
      "Rounded output:\n",
      " [0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# 1.(c) Implementing model without libraries\n",
    "# Solving XOR with binary crossentropy loss function and A1 = Hyperbolic Tangent and A2 = Sigmoid\n",
    "# A1, A2 are activation functions for layer 1 (H1) and layer 2 (H2) respectively\n",
    "from abc import ABC\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.network_layers = []\n",
    "        self.loss = None\n",
    "        self.derivative_loss = None\n",
    "\n",
    "    # Function to apply binary crossentropy loss\n",
    "    def apply(self, loss, derivative_loss):\n",
    "        self.loss = loss\n",
    "        self.derivative_loss = derivative_loss\n",
    "\n",
    "    # Function to fit the model on the training set for n number of epochs and specified learning rate\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        example_set = len(x_train)\n",
    "        for i in range(epochs):\n",
    "            for j in range(example_set):\n",
    "                output = x_train[j]\n",
    "                for layer in self.network_layers:\n",
    "                    output = layer.f_prop(output)\n",
    "\n",
    "                error = self.derivative_loss(y_train[j], output)\n",
    "                for layer in reversed(self.network_layers):\n",
    "                    error = layer.b_prop(error, learning_rate)\n",
    "\n",
    "    # Function to add a layer to the network\n",
    "    def add_layer(self, layer):\n",
    "        self.network_layers.append(layer)\n",
    "\n",
    "    # Function to predict y_pred\n",
    "    def predict(self, ip_data):\n",
    "\n",
    "        example_set = len(ip_data)\n",
    "        result = []\n",
    "        for i in range(example_set):\n",
    "            output = ip_data[i]\n",
    "            for layer in self.network_layers:\n",
    "                output = layer.f_prop(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "\n",
    "class NeuralLayer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def f_prop(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def b_prop(self, op_err, learning_rate):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Hidden(NeuralLayer, ABC):\n",
    "    def __init__(self, ip_size, op_size):\n",
    "        self.weight_values = np.random.rand(ip_size, op_size)\n",
    "        self.bias_values = np.random.rand(1, op_size)\n",
    "\n",
    "    # Function to perform forward propagation\n",
    "    def f_prop(self, ip_data):\n",
    "        self.input = ip_data\n",
    "        self.output = np.dot(self.input, self.weight_values) + self.bias_values\n",
    "        return self.output\n",
    "\n",
    "    # Function to perform backward propagation and adjust the weight_values and bias_values\n",
    "    def b_prop(self, op_err, learning_rate):\n",
    "        ip_err = np.dot(op_err, self.weight_values.T)\n",
    "        weight_values_error = np.dot(self.input.T, op_err)\n",
    "        self.weight_values -= learning_rate * weight_values_error\n",
    "        self.bias_values -= learning_rate * op_err\n",
    "        return ip_err\n",
    "\n",
    "class Activation(NeuralLayer, ABC):\n",
    "    def __init__(self, activation, derivative_activation):\n",
    "        self.activation = activation\n",
    "        self.derivative_activation = derivative_activation\n",
    "\n",
    "    # Function for activation in forward propagation\n",
    "    def f_prop(self, ip_data):\n",
    "        self.input = ip_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Function for derivative of activation in backward propagation (for finding gradient to adjust weight_values and bias_values)\n",
    "    def b_prop(self, op_err, learning_rate=None):\n",
    "        return self.derivative_activation(self.input) * op_err\n",
    "\n",
    "# Functions for loss, loss derivative, activation functions and their derivatives\n",
    "def binary_crossentropy_loss(y_true, y_pred):\n",
    "    first_half = -np.multiply(y_true, np.log(y_pred))\n",
    "    second_half = -np.multiply((1-y_true), np.log(1-y_pred))\n",
    "    return np.mean(first_half + second_half)\n",
    "\n",
    "def d_binary_crossentropy_loss(y_true, y_pred):\n",
    "    first_half = np.divide(y_true,y_pred)\n",
    "    second_half = np.divide((1-y_true),(1-y_pred))\n",
    "    return -first_half + second_half\n",
    "\n",
    "def hyperbolic_tangent_function(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_hyperbolic_tangent_function(x):\n",
    "    return 1-(np.tanh(x)**2)\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid_function(x):\n",
    "    z = 1/(1+np.exp(-x))\n",
    "    dz = z * (1-z)\n",
    "    return dz\n",
    "\n",
    "# Given dataset for XOR\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# Creating a neural network\n",
    "solve_xor = NeuralNetwork()\n",
    "solve_xor.add_layer(Hidden(2, 2))\n",
    "solve_xor.add_layer(Activation(hyperbolic_tangent_function, d_hyperbolic_tangent_function))\n",
    "solve_xor.add_layer(Hidden(2, 1))\n",
    "solve_xor.add_layer(Activation(sigmoid_function, d_sigmoid_function))\n",
    "solve_xor.apply(binary_crossentropy_loss, d_binary_crossentropy_loss)\n",
    "solve_xor.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# Predicting data\n",
    "output_data = solve_xor.predict(x_train)\n",
    "\n",
    "output_values = []\n",
    "for i in output_data:\n",
    "    output_list_value = [value for values in i for value in values] # Flattening list\n",
    "    output_values.append(output_list_value[0])\n",
    "\n",
    "final_output_values = []\n",
    "\n",
    "for output_value in output_values:\n",
    "    final_output_value = round(output_value)\n",
    "    final_output_values.append(final_output_value)\n",
    "\n",
    "print(\"Raw output:\\n\", output_data, \"\\nRounded output:\\n\", final_output_values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}